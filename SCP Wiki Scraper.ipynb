{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1908891d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports 和全局设置 cell\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re\n",
    "from functools import lru_cache\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# 优先使用 lxml，加速解析；不可用则回退\n",
    "try:\n",
    "    import lxml  # 仅用于检查是否可用\n",
    "    BS_PARSER = 'lxml'\n",
    "except Exception:\n",
    "    BS_PARSER = 'html.parser'\n",
    "\n",
    "# 复用 Session + 重试策略 + 连接池\n",
    "session = requests.Session()\n",
    "retry_strategy = Retry(\n",
    "    total=3,\n",
    "    connect=3,\n",
    "    read=3,\n",
    "    backoff_factor=0.5,\n",
    "    status_forcelist=[429, 500, 502, 503, 504],\n",
    "    allowed_methods=[\"GET\"],\n",
    "    raise_on_status=False,\n",
    ")\n",
    "adapter = HTTPAdapter(max_retries=retry_strategy, pool_connections=20, pool_maxsize=20)\n",
    "session.mount(\"http://\", adapter)\n",
    "session.mount(\"https://\", adapter)\n",
    "session.headers.update({\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "})\n",
    "\n",
    "# 可控日志\n",
    "VERBOSE = False\n",
    "def vprint(*args, **kwargs):\n",
    "    if VERBOSE:\n",
    "        print(*args, **kwargs)\n",
    "\n",
    "# 在第一个cell中添加预编译正则\n",
    "# 预编译正则\n",
    "RE_REDACT = re.compile('\\u2588+')\n",
    "RE_WS = re.compile(r'\\s+')\n",
    "RE_CLEAN_NAME = re.compile(r'[·•].*$')  # 新增：清理名称的正则\n",
    "\n",
    "IMAGE_EXTS = ('.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg')\n",
    "\n",
    "base_url = \"http://scp-wiki-cn.wikidot.com/scp-\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4cb75e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_images(scp_id: int):\n",
    "    \"\"\"分析页面中的图片并打印与项目相关的图片URL\"\"\"\n",
    "    urls = get_scp_images(scp_id)\n",
    "    print(f\"SCP-{scp_id:03d} 相关图片共 {len(urls)} 张：\")\n",
    "    for i, u in enumerate(urls, 1):\n",
    "        print(f\"{i}. {u}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d03afbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract_urls_from_img(img_tag):\n",
    "    \"\"\"从单个 <img> 标签收集所有可能的图片URL\"\"\"\n",
    "    urls = set()\n",
    "    # 直接 src\n",
    "    src = (img_tag.get('src') or '').strip()\n",
    "    if src:\n",
    "        urls.add(src)\n",
    "    # 懒加载常见属性\n",
    "    for attr in ('data-src', 'data-image'):\n",
    "        val = (img_tag.get(attr) or '').strip()\n",
    "        if val:\n",
    "            urls.add(val)\n",
    "    # srcset: 可能包含多条，以逗号分隔，形如 \"url w, url2 w\"\n",
    "    srcset = (img_tag.get('srcset') or '').strip()\n",
    "    if srcset:\n",
    "        for part in srcset.split(','):\n",
    "            p = part.strip().split(' ')[0].strip()\n",
    "            if p:\n",
    "                urls.add(p)\n",
    "    return list(urls)\n",
    "\n",
    "def _normalize_and_filter_urls(urls, page_url):\n",
    "    \"\"\"标准化为绝对URL并过滤非图片/无效链接\"\"\"\n",
    "    abs_urls = []\n",
    "    for u in urls:\n",
    "        full = urljoin(page_url, u)\n",
    "        # 只接受 http/https\n",
    "        if not (full.startswith('http://') or full.startswith('https://')):\n",
    "            continue\n",
    "        # 过滤常见静态文件外的资源，限制为图片扩展名\n",
    "        lower = full.lower()\n",
    "        if any(lower.endswith(ext) for ext in IMAGE_EXTS):\n",
    "            abs_urls.append(full)\n",
    "    return abs_urls\n",
    "\n",
    "def _is_relevant_image(url, alt, title, scp_id_formatted):\n",
    "    \"\"\"根据URL/alt/title 判断是否与SCP项目相关（优化版）\"\"\"\n",
    "    # 使用更高效的字符串操作\n",
    "    url_lower = url.lower() if url else ''\n",
    "    \n",
    "    # 优先检查最可能的匹配\n",
    "    if f\"scp-{scp_id_formatted}\" in url_lower:\n",
    "        return True\n",
    "    if 'scp' in url_lower:\n",
    "        return True\n",
    "    \n",
    "    # 检查alt和title（通常较短，检查成本低）\n",
    "    if alt or title:\n",
    "        alt_lower = alt.lower() if alt else ''\n",
    "        title_lower = title.lower() if title else ''\n",
    "        if 'scp' in alt_lower or 'scp' in title_lower:\n",
    "            return True\n",
    "    \n",
    "    return False\n",
    "\n",
    "def analyze_images(scp_id: int):\n",
    "    \"\"\"分析页面中的图片并打印与项目相关的图片URL\"\"\"\n",
    "    _id = harmonize_id(scp_id)\n",
    "    page_url = base_url + _id\n",
    "    try:\n",
    "        resp = session.get(page_url, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.content, BS_PARSER)\n",
    "        page_content = soup.find('div', id='page-content')\n",
    "        urls = extract_images_from_soup(soup, page_content, scp_id, resp.url)\n",
    "    except Exception as e:\n",
    "        vprint(f\"获取页面失败 {page_url}: {e}\")\n",
    "        urls = []\n",
    "    \n",
    "    print(f\"SCP-{scp_id:03d} 相关图片共 {len(urls)} 张：\")\n",
    "    for i, u in enumerate(urls, 1):\n",
    "        print(f\"{i}. {u}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b97f843",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "b5d8291d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relax_key(current_key: str) -> str:\n",
    "    \"\"\"将中文字段名映射为英文键名\"\"\"\n",
    "    key_mapping = {\n",
    "        '项目编号：': 'id',\n",
    "        '项目等级：': 'class', \n",
    "        '描述：': 'description',\n",
    "        '特殊收容措施：': 'containment',\n",
    "        '附录：': 'addendum',\n",
    "        '实验记录：': 'experiment_log',\n",
    "        '访谈记录：': 'interview_log',\n",
    "        '事件记录：': 'incident_log',\n",
    "        '更新记录：': 'update_log',\n",
    "        '历史：': 'history',\n",
    "        '发现：': 'discovery'\n",
    "    }\n",
    "    \n",
    "    # 直接映射\n",
    "    if current_key in key_mapping:\n",
    "        return key_mapping[current_key]\n",
    "    \n",
    "    # 处理带冒号的字段\n",
    "    if current_key.endswith('：') and len(current_key) > 1:\n",
    "        return current_key[:-1].lower().replace(' ', '_')\n",
    "    \n",
    "    # 默认处理\n",
    "    return current_key.lower().replace(' ', '_') if current_key else 'unknown'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "35698c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def harmonize_id(_id: int) -> str:\n",
    "    # 使用zfill()方法将数字填充为3位字符串\n",
    "    # zfill性能优于字符串拼接,因为它是C实现的内置方法\n",
    "    return str(_id).zfill(3)\n",
    "\n",
    "def get_series_number(_id: int) -> int:\n",
    "    \"\"\"根据项目编号计算所属系列\n",
    "    \n",
    "    Args:\n",
    "        _id: SCP项目编号\n",
    "        \n",
    "    Returns:\n",
    "        系列编号 (1-9)\n",
    "    \"\"\"\n",
    "    # 系列计算公式：项目编号除以1000取整+1\n",
    "    series = (_id // 1000) + 1\n",
    "    return max(1, min(series, 9))  # 限制在1-9范围内\n",
    "\n",
    "def get_series_url(series_number: int) -> str:\n",
    "    \"\"\"根据系列编号获取系列页面URL\n",
    "    \n",
    "    Args:\n",
    "        series_number: 系列编号 (1-9)\n",
    "        \n",
    "    Returns:\n",
    "        系列页面的URL\n",
    "    \"\"\"\n",
    "    base_series_url = \"http://scp-wiki-cn.wikidot.com/scp-series\"\n",
    "    if series_number == 1:\n",
    "        return base_series_url\n",
    "    else:\n",
    "        return f\"{base_series_url}-{series_number}\"\n",
    "\n",
    "@lru_cache(maxsize=16)\n",
    "def fetch_series_page(series_number: int) -> bytes:\n",
    "    \"\"\"缓存系列页 HTML，减少重复请求\"\"\"\n",
    "    url = get_series_url(series_number)\n",
    "    try:\n",
    "        resp = session.get(url, timeout=10)\n",
    "        resp.raise_for_status()\n",
    "        return resp.content\n",
    "    except Exception as e:\n",
    "        vprint(f\"获取系列页失败 {url}: {e}\")\n",
    "        return b\"\"\n",
    "\n",
    "def extract_images_from_soup(soup, page_content, scp_id, page_url):\n",
    "    \"\"\"从已解析的soup中提取图片，避免重复请求和解析\"\"\"\n",
    "    if not page_content:\n",
    "        return []\n",
    "\n",
    "    scp_id_formatted = harmonize_id(scp_id)\n",
    "    ordered, seen = [], set()\n",
    "\n",
    "    # 只在主体内容区域中找图片\n",
    "    imgs = page_content.find_all('img')\n",
    "    for img in imgs:\n",
    "        candidates = _extract_urls_from_img(img)\n",
    "        normalized = _normalize_and_filter_urls(candidates, page_url)\n",
    "        alt = img.get('alt') or ''\n",
    "        title = img.get('title') or ''\n",
    "        for u in normalized:\n",
    "            if _is_relevant_image(u, alt, title, scp_id_formatted):\n",
    "                if u not in seen:\n",
    "                    seen.add(u)\n",
    "                    ordered.append(u)\n",
    "\n",
    "    return ordered\n",
    "\n",
    "# 在extract_images_from_soup函数之后添加新的代码单元格\n",
    "def extract_tags_from_soup(soup, page_content):\n",
    "    \"\"\"从已解析的soup中提取SCP项目的标签信息，并过滤掉不需要的标签\"\"\"\n",
    "    if not soup:\n",
    "        return []\n",
    "    \n",
    "    tags = []\n",
    "    \n",
    "    # 定义需要过滤的标签（不区分大小写）\n",
    "    filtered_tags = {\n",
    "        'scp', 'safe', 'euclid', 'keter', 'thaumiel', 'apollyon', \n",
    "        'archon', 'neutralized', 'explained', 'decommissioned'\n",
    "    }\n",
    "    \n",
    "    # 查找页面标签容器\n",
    "    page_tags_div = soup.find('div', class_='page-tags')\n",
    "    if page_tags_div:\n",
    "        # 在标签容器中查找所有链接\n",
    "        tag_links = page_tags_div.find_all('a')\n",
    "        for link in tag_links:\n",
    "            tag_text = link.get_text().strip()\n",
    "            if tag_text and tag_text.lower() not in filtered_tags and tag_text not in tags:\n",
    "                tags.append(tag_text)\n",
    "    \n",
    "    # 如果没有找到标签容器，尝试在页面内容中查找标签链接\n",
    "    if not tags and page_content:\n",
    "        # 查找可能的标签链接（通常以/tag/开头）\n",
    "        tag_links = page_content.find_all('a', href=True)\n",
    "        for link in tag_links:\n",
    "            href = link.get('href', '')\n",
    "            if '/tag/' in href:\n",
    "                tag_text = link.get_text().strip()\n",
    "                if tag_text and tag_text.lower() not in filtered_tags and tag_text not in tags:\n",
    "                    tags.append(tag_text)\n",
    "    \n",
    "    return tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "series_name_function",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scp_name_from_series(_id: int) -> str:\n",
    "    \"\"\"从系列页面获取SCP项目名称（优化版）\"\"\"\n",
    "    try:\n",
    "        series_number = get_series_number(_id)\n",
    "        content = fetch_series_page(series_number)\n",
    "        if not content:\n",
    "            return \"\"\n",
    "\n",
    "        soup = BeautifulSoup(content, BS_PARSER)\n",
    "\n",
    "        # 查找包含SCP编号的链接\n",
    "        scp_id_formatted = harmonize_id(_id)\n",
    "        scp_link_pattern = f\"scp-{scp_id_formatted}\"\n",
    "\n",
    "        # 在页面中查找匹配的SCP链接\n",
    "        links = soup.find_all('a', href=True)\n",
    "        for link in links:\n",
    "            if scp_link_pattern in link.get('href', ''):\n",
    "                # 获取链接文本，通常格式为 \"SCP-XXX - 名称\"\n",
    "                link_text = link.get_text().strip()\n",
    "                if ' - ' in link_text:\n",
    "                    # 提取名称部分（去掉SCP-XXX部分）\n",
    "                    name_part = link_text.split(' - ', 1)[1].strip()\n",
    "                    return name_part\n",
    "\n",
    "        # 如果没有找到，尝试在文本中直接搜索\n",
    "        page_text = soup.get_text()\n",
    "        lines = page_text.split('\\n')\n",
    "        for line in lines:\n",
    "            if f\"SCP-{scp_id_formatted}\" in line and ' - ' in line:\n",
    "                parts = line.split(' - ', 1)\n",
    "                if len(parts) > 1:\n",
    "                    name_part = parts[1].strip()\n",
    "                    # 使用预编译正则清理可能的额外字符\n",
    "                    name_part = RE_CLEAN_NAME.sub('', name_part).strip()\n",
    "                    if name_part:\n",
    "                        return name_part\n",
    "\n",
    "        return \"\"  # 未找到名称\n",
    "\n",
    "    except Exception as e:\n",
    "        vprint(f\"获取SCP-{_id}名称时出错: {str(e)}\")\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "0754e0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affix_additional(results):\n",
    "    \"\"\"整理提取结果，将标准字段和附加信息分开\"\"\"\n",
    "    _results = results.copy()\n",
    "    _additional_info = {}\n",
    "    \n",
    "    # 扩展标准字段列表\n",
    "    standard_fields = [\n",
    "        'id', 'class', 'description', 'containment', 'addendum',\n",
    "        'experiment_log', 'interview_log', 'incident_log', \n",
    "        'update_log', 'history', 'discovery', 'error', 'warning',\n",
    "        'series', 'name', 'images', 'tags'  # 新增系列、名称、图片、标签字段\n",
    "    ]\n",
    "    \n",
    "    to_delete = []\n",
    "    \n",
    "    for key in _results:\n",
    "        if key not in standard_fields:\n",
    "            _additional_info[key] = _results[key]\n",
    "            to_delete.append(key)\n",
    "    \n",
    "    # 删除已移动到附加信息的字段\n",
    "    for key in to_delete:\n",
    "        del _results[key]\n",
    "    \n",
    "    # 只有当有附加信息时才添加more_info字段\n",
    "    if _additional_info:\n",
    "        _results['more_info'] = _additional_info\n",
    "    \n",
    "    return _results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "5a34b87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 函数：scrape_scp（优化连接、解析与日志）\n",
    "def scrape_scp(id):\n",
    "    \"\"\"改进的SCP爬取函数，增强错误处理和解析逻辑，包含系列、名称与图片信息\"\"\"\n",
    "    result_dict = {}\n",
    "    _id = harmonize_id(id)\n",
    "    url = base_url + _id\n",
    "\n",
    "    # 获取项目系列和名称\n",
    "    series_number = get_series_number(id)\n",
    "    scp_name = get_scp_name_from_series(id)\n",
    "\n",
    "    # 添加系列和名称到结果中\n",
    "    result_dict['series'] = series_number\n",
    "    if scp_name:\n",
    "        result_dict['name'] = scp_name\n",
    "\n",
    "    try:\n",
    "        # 复用全局 session（含UA与重试）\n",
    "        response = session.get(url, timeout=10)\n",
    "        response.raise_for_status()  # 检查HTTP状态码\n",
    "        vprint(f\"成功访问: {url}\")\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        vprint(f\"请求失败 {url}: {str(e)}\")\n",
    "        return {'error': f'请求失败: {str(e)}'}\n",
    "\n",
    "    soup = BeautifulSoup(response.content, BS_PARSER)\n",
    "    page_content = soup.find('div', id='page-content')\n",
    "\n",
    "    if not page_content:\n",
    "        vprint(f\"未找到页面内容: {url}\")\n",
    "        return {'error': '未找到页面内容'}\n",
    "\n",
    "    # 获取所有可能包含信息的元素，不仅仅是p标签\n",
    "    elements = page_content.find_all(['p', 'div', 'blockquote'])\n",
    "\n",
    "    current_key = None\n",
    "    current_value = ''\n",
    "    stop_indicators = ['«', '‹', '附录', '实验记录', '访谈记录', '事件记录']\n",
    "\n",
    "    for element in elements:\n",
    "        element_text = element.get_text().strip()\n",
    "\n",
    "        # 跳过空元素\n",
    "        if not element_text:\n",
    "            continue\n",
    "\n",
    "        # 检查是否应该停止解析主要内容\n",
    "        if any(indicator in element_text for indicator in stop_indicators[:2]):\n",
    "            # 如果遇到导航符号，停止解析\n",
    "            if element_text.startswith(('«', '‹')):\n",
    "                break\n",
    "\n",
    "        # 查找强调标签（字段名）\n",
    "        strong_tag = element.find('strong')\n",
    "\n",
    "        if strong_tag:\n",
    "            # 保存上一个字段\n",
    "            if current_key:\n",
    "                _k = relax_key(current_key)\n",
    "                current_value = RE_REDACT.sub('[REDACTED]', current_value)\n",
    "                current_value = RE_WS.sub(' ', current_value)  # 规范化空白字符\n",
    "                result_dict[_k] = current_value.strip()\n",
    "\n",
    "            # 开始新字段\n",
    "            current_key = strong_tag.get_text().strip()\n",
    "            # 获取字段值（去掉字段名部分）\n",
    "            current_value = element_text[len(current_key):].strip()\n",
    "\n",
    "        else:\n",
    "            # 如果当前有字段在处理，添加到其值中\n",
    "            if current_key:\n",
    "                if current_value and not current_value.endswith(' '):\n",
    "                    current_value += ' '\n",
    "                current_value += element_text\n",
    "\n",
    "    # 处理最后一个字段\n",
    "    if current_key:\n",
    "        _k = relax_key(current_key)\n",
    "        current_value = RE_REDACT.sub('[REDACTED]', current_value)\n",
    "        current_value = RE_WS.sub(' ', current_value)\n",
    "        result_dict[_k] = current_value.strip()\n",
    "\n",
    "    # 优化：复用已获取的soup和page_content来提取图片，避免重复请求\n",
    "    images = extract_images_from_soup(soup, page_content, id, response.url)\n",
    "    if images:\n",
    "        result_dict['images'] = images\n",
    "\n",
    "    # 提取标签信息\n",
    "    tags = extract_tags_from_soup(soup, page_content)\n",
    "    if tags:\n",
    "        result_dict['tags'] = tags\n",
    "\n",
    "    # 如果没有提取到任何有效字段\n",
    "    if not result_dict or all(key == 'error' for key in result_dict.keys()):\n",
    "        vprint(f\"警告: 未能提取到有效字段 {url}\")\n",
    "        result_dict['warning'] = '未能提取到标准SCP字段'\n",
    "\n",
    "    result_dict = affix_additional(result_dict)\n",
    "    return result_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0f443082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== 爬取完成 ===\n",
      "成功: 1 个\n",
      "失败: 0 个\n"
     ]
    }
   ],
   "source": [
    "# 存储SCP条目的字典\n",
    "db = {}\n",
    "\n",
    "# change these if you want mass scraping\n",
    "start = 49\n",
    "end = 49\n",
    "\n",
    "# 检查参数有效性\n",
    "if not isinstance(start, int) or not isinstance(end, int):\n",
    "    raise ValueError(\"起始和结束编号必须为整数\")\n",
    "    \n",
    "if start > end:\n",
    "    raise ValueError(\"起始编号不能大于结束编号\")\n",
    "\n",
    "# 使用改进的批量爬取，增加错误处理\n",
    "failed_ids = []\n",
    "success_count = 0\n",
    "\n",
    "for i in range(start, end + 1):\n",
    "    try:\n",
    "        result = scrape_scp(i)\n",
    "        if 'error' not in result:\n",
    "            db[str(i)] = result\n",
    "            success_count += 1\n",
    "        else:\n",
    "            failed_ids.append(i)\n",
    "            print(f\"SCP-{i:03d} 爬取失败: {result.get('error', '未知错误')}\")\n",
    "    except Exception as e:\n",
    "        failed_ids.append(i)\n",
    "        print(f\"SCP-{i:03d} 爬取异常: {str(e)}\")\n",
    "\n",
    "print(f\"\\n=== 爬取完成 ===\")\n",
    "print(f\"成功: {success_count} 个\")\n",
    "print(f\"失败: {len(failed_ids)} 个\")\n",
    "if failed_ids:\n",
    "    print(f\"失败的ID: {failed_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5acea7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary written to scp_database_cn.json successfully.\n"
     ]
    }
   ],
   "source": [
    "file_path = \"scp_database_cn.json\"\n",
    "\n",
    "# Write dictionary to JSON file\n",
    "with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(db, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"Dictionary written to {file_path} successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
